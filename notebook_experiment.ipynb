{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c72b30",
   "metadata": {},
   "source": [
    "# Clone full repo to copy aditional python files if running on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if in colab\n",
    "!RunningInCOLAB = 'google.colab' in str(get_ipython()) if hasattr(__builtins__,'__IPYTHON__') else False\n",
    "# Continue only if running on Google Colab\n",
    "![[ ! RunningInCOLAB ]] && exit\n",
    "\n",
    "# clone repo and move to current working dir\n",
    "!git clone https://github.com/Juliano-rb/experiments_fault_injection_mlaas.git repo\n",
    "!rsync -av repo/ .\n",
    "!rm -rf repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c39734",
   "metadata": {},
   "source": [
    "# Installing dependencies with pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26aae54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: jupyter-client\n",
      "    Found existing installation: jupyter_client 8.5.0\n",
      "    Uninstalling jupyter_client-8.5.0:\n",
      "      Successfully uninstalled jupyter_client-8.5.0\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 6.26.0\n",
      "    Uninstalling ipykernel-6.26.0:\n",
      "      Successfully uninstalled ipykernel-6.26.0\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 4.0.9\n",
      "    Uninstalling widgetsnbextension-4.0.9:\n",
      "      Successfully uninstalled widgetsnbextension-4.0.9\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 8.1.1\n",
      "    Uninstalling ipywidgets-8.1.1:\n",
      "      Successfully uninstalled ipywidgets-8.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "launchpadlib 1.10.13 requires testresources, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.8.6 aiosignal-1.3.1 annotated-types-0.6.0 anyio-3.7.1 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 async-timeout-4.0.3 attrs-23.1.0 azure-ai-textanalytics-5.1.0 azure-common-1.1.28 azure-core-1.29.5 babel-2.13.1 backoff-2.2.1 beautifulsoup4-4.8.2 bleach-6.1.0 boto3-1.24.0 botocore-1.27.96 cachetools-5.3.2 certifi-2023.7.22 cffi-1.16.0 charset-normalizer-3.3.2 click-8.1.7 cohere-4.32 contourpy-1.1.1 cycler-0.12.1 decorator-4.0.2 defusedxml-0.7.1 distro-1.8.0 entrypoints-0.4 et-xmlfile-1.1.0 exceptiongroup-1.1.3 fastapi-0.104.1 fastavro-1.8.2 fastjsonschema-2.18.1 fonttools-4.44.0 frozenlist-1.4.0 fsspec-2023.6.0 gdown-4.7.1 gensim-4.2.0 google-api-core-2.12.0 google-auth-2.17.3 google-cloud-language-2.4.2 googleapis-common-protos-1.61.0 grpcio-1.59.2 grpcio-status-1.59.2 h11-0.14.0 httpcore-1.0.1 httpx-0.25.1 huggingface-hub-0.18.0 idna-3.4 importlib-resources-5.9.0 ipykernel-5.5.6 ipympl-0.9.2 ipython-7.34.0 ipython-genutils-0.2.0 ipywidgets-7.7.2 isodate-0.6.1 jinja2-3.1.2 jmespath-1.0.1 joblib-1.3.2 json5-0.9.14 jsonschema-4.19.2 jsonschema-specifications-2023.7.1 jupyter-client-7.4.9 jupyter-server-1.24.0 jupyterlab-3.4.3 jupyterlab-pygments-0.2.2 jupyterlab-server-2.25.0 jupyterlab-widgets-1.1.7 kaleido-0.2.1 kiwisolver-1.4.5 markupsafe-2.1.3 matplotlib-3.7.3 mistune-3.0.2 msrest-0.7.1 multidict-6.0.4 nbclassic-0.5.6 nbclient-0.8.0 nbconvert-7.10.0 nbformat-5.9.2 nlpaug-1.1.11 nltk-3.7 notebook-6.5.5 notebook-shim-0.2.3 numpy-1.24.4 oauthlib-3.2.2 openai-1.1.1 openpyxl-3.0.10 pandas-1.5.3 pandocfilters-1.5.0 pexpect-4.8.0 pillow-8.3.2 pkgutil-resolve-name-1.3.10 plotly-5.11.0 prometheus-client-0.18.0 proto-plus-1.22.3 protobuf-4.25.0 ptyprocess-0.7.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycparser-2.21 pydantic-2.4.2 pydantic-core-2.10.1 pyparsing-3.1.1 pysocks-1.7.1 python-multipart-0.0.6 pytz-2023.3.post1 pyyaml-6.0.1 pyzmq-24.0.1 referencing-0.30.2 regex-2023.10.3 requests-2.31.0 requests-oauthlib-1.3.1 rpds-py-0.12.0 rsa-4.9 s3transfer-0.6.2 scikit-learn-1.3.2 scipy-1.10.1 seaborn-0.12.1 send2trash-1.8.2 six-1.16.0 smart-open-6.4.0 sniffio-1.3.0 soupsieve-2.5 starlette-0.27.0 tenacity-8.2.3 terminado-0.17.1 threadpoolctl-3.2.0 tiktoken-0.5.1 tinycss2-1.2.1 tokenizers-0.12.1 tornado-6.3.2 tqdm-4.66.1 transformers-4.20.1 urllib3-1.26.18 uvicorn-0.24.0.post1 webencodings-0.5.1 websocket-client-1.6.4 widgetsnbextension-3.6.6 xlrd-2.0.1 yarl-1.9.2 zipp-3.17.0\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8c2b4-7d16-4424-a90c-7ed7bfb8b205",
   "metadata": {},
   "source": [
    "## Download the pre-trained ``glove.twitter`` word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d410856-5d7f-431c-af3e-ab89fb7df372",
   "metadata": {
    "tags": [
     "common"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipywidgets in /home/julianoro/.local/lib/python3.8/site-packages (7.7.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/julianoro/.local/lib/python3.8/site-packages (from ipywidgets) (5.5.6)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /home/julianoro/.local/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/julianoro/.local/lib/python3.8/site-packages (from ipywidgets) (5.13.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /home/julianoro/.local/lib/python3.8/site-packages (from ipywidgets) (3.6.6)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /home/julianoro/.local/lib/python3.8/site-packages (from ipywidgets) (7.34.0)\n",
      "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in /home/julianoro/.local/lib/python3.8/site-packages (from ipywidgets) (1.1.7)\n",
      "Requirement already satisfied: jupyter-client in /home/julianoro/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.9)\n",
      "Requirement already satisfied: tornado>=4.2 in /home/julianoro/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/julianoro/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (68.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/julianoro/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: decorator in /home/julianoro/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.0.2)\n",
      "Requirement already satisfied: pickleshare in /home/julianoro/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/julianoro/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: pygments in /home/julianoro/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.16.1)\n",
      "Requirement already satisfied: backcall in /home/julianoro/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /home/julianoro/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/julianoro/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/julianoro/.local/lib/python3.8/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/julianoro/.local/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: jinja2 in /home/julianoro/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.2)\n",
      "Requirement already satisfied: pyzmq<25,>=17 in /home/julianoro/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.0.1)\n",
      "Requirement already satisfied: argon2-cffi in /home/julianoro/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in /home/julianoro/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.5.0)\n",
      "Requirement already satisfied: nbformat in /home/julianoro/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.9.2)\n",
      "Requirement already satisfied: nbconvert>=5 in /home/julianoro/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.10.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /home/julianoro/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.8)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /home/julianoro/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/julianoro/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.17.1)\n",
      "Requirement already satisfied: prometheus-client in /home/julianoro/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /home/julianoro/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.6)\n",
      "Requirement already satisfied: entrypoints in /home/julianoro/.local/lib/python3.8/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/julianoro/.local/lib/python3.8/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/julianoro/.local/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/julianoro/.local/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.9)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/julianoro/.local/lib/python3.8/site-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.11.0)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in /home/julianoro/.local/lib/python3.8/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /home/julianoro/.local/lib/python3.8/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/julianoro/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.8.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/julianoro/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /home/julianoro/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /home/julianoro/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.8.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/julianoro/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /home/julianoro/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.1.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/julianoro/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/julianoro/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.0)\n",
      "Requirement already satisfied: packaging in /home/julianoro/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/julianoro/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /home/julianoro/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in /home/julianoro/.local/lib/python3.8/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.18.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/julianoro/.local/lib/python3.8/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.19.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/julianoro/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.2->jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/julianoro/.local/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: webencodings in /home/julianoro/.local/lib/python3.8/site-packages (from bleach!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/julianoro/.local/lib/python3.8/site-packages (from importlib-metadata>=3.6->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.17.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/julianoro/.local/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /home/julianoro/.local/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/julianoro/.local/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2023.7.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /home/julianoro/.local/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/julianoro/.local/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/julianoro/.local/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.12.0)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in /home/julianoro/.local/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
      "Requirement already satisfied: websocket-client in /home/julianoro/.local/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.4)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/julianoro/.local/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/julianoro/.local/lib/python3.8/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.5)\n",
      "Requirement already satisfied: idna>=2.8 in /home/julianoro/.local/lib/python3.8/site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/julianoro/.local/lib/python3.8/site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/julianoro/.local/lib/python3.8/site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.3)\n",
      "Requirement already satisfied: pycparser in /home/julianoro/.local/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\n",
      "file  models/glove.twitter.27B.100d.txt  already exists.\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install ipywidgets\n",
    "import urllib.request\n",
    "from os.path import exists\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "progress = None\n",
    "def show_progress(block_num, block_size, total_size):\n",
    "    global progress\n",
    "    if not progress :\n",
    "        progress = widgets.FloatProgress(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=total_size,\n",
    "            step=0.1,\n",
    "            description='Downloading',\n",
    "            bar_style='info',\n",
    "            orientation='horizontal'\n",
    "        )\n",
    "        display(progress)\n",
    "        \n",
    "    downloaded = (block_num * block_size)\n",
    "    print(block_num * block_size, \"/\", total_size,\"\\r\", end=\"\")\n",
    "    \n",
    "    progress.value = downloaded\n",
    "\n",
    "model_path = \"models/glove.twitter.27B.100d.txt\"\n",
    "word_embedding_url = \"https://huggingface.co/Juliano/fault_injection_mlaas/resolve/main/glove.twitter.27B.100d.txt\"\n",
    "\n",
    "file_exists = exists(model_path)\n",
    "\n",
    "if file_exists :\n",
    "    print(\"file \", model_path, \" already exists.\")\n",
    "else:\n",
    "    filename = \"models\"\n",
    "    os.makedirs(filename, exist_ok=True)\n",
    "    urllib.request.urlretrieve(word_embedding_url, model_path, show_progress)\n",
    "    print(\"File downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e142e4-828c-42c5-9e9b-df0797ee7462",
   "metadata": {},
   "source": [
    "## Importing and Mocking MLaaS providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "755e5348-bbf8-452c-9801-3ced92238e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlaas_providers import providers as ml_providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fee9d3-c191-4629-805d-1ea45992965d",
   "metadata": {},
   "source": [
    "Run if you want to use mocked providers instead real ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85da9cd7-5606-45fb-af7f-b76f30ded8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_providers.amazon = ml_providers.return_mock_of(ml_providers.amazon)\n",
    "ml_providers.google = ml_providers.return_mock_of(ml_providers.google)\n",
    "ml_providers.microsoft = ml_providers.return_mock_of(ml_providers.microsoft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bded844-ed3f-4355-a758-0491eb1f624e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `Experiment 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46011a-b681-4645-abe3-cd8385908f3d",
   "metadata": {},
   "source": [
    "## Importing code modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42a5089e-6025-4bcf-9260-22425a249776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from typing import List\n",
    "from mlaas_providers.providers import read_dataset\n",
    "from noise_insertion.utils import save_data_to_file\n",
    "from data_sampling.data_sampling import DataSampling\n",
    "from noise_insertion.percent_insertion import noises\n",
    "from noise_insertion import noise_insertion\n",
    "from visualization import visualization\n",
    "from progress import progress_manager\n",
    "from metrics import metrics\n",
    "import ipywidgets as widgets\n",
    "data_sampling = DataSampling()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544ecda-802d-4cf9-9919-ed7800d8a5f6",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cad8723-fe62-4ce3-8a45-68f9269ea53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 99\n",
    "\n",
    "noise_list =[\n",
    "    noises.Keyboard,\n",
    "    noises.OCR,\n",
    "    noises.RandomCharReplace,\n",
    "    noises.CharSwap,\n",
    "    noises.WordSwap,\n",
    "    noises.WordSplit,\n",
    "    noises.Antonym,\n",
    "    noises.Synonym,\n",
    "    noises.Spelling,\n",
    "    noises.TfIdfWord,\n",
    "    noises.WordEmbeddings,\n",
    "    noises.ContextualWordEmbs,\n",
    "]\n",
    "\n",
    "noise_level=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8c81e4-cfdb-4773-b0f9-90e6e6e326d2",
   "metadata": {},
   "source": [
    "## Running the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c39d25-848c-4897-9179-d8309374cd38",
   "metadata": {},
   "source": [
    "To continue from previously ongoing progress insert the name of a /outputs/experiment1 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec350fd-7a1a-4e44-98d8-76c8861ded3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f391dd07495a4910afce145c69d403f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Continue from', placeholder='Type the name of a /outputs/experiment1 folder to…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "continue_widget = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Type the name of a /outputs/experiment1 folder to continue from',\n",
    "    description='Continue from',\n",
    "    disabled=False\n",
    ")\n",
    "continue_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8fe718-b947-43cd-8aa2-0ffc21795c5c",
   "metadata": {
    "scrolled": true,
    "tags": [
     "experiment1"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be stored at:  ./outputs/experiment1/size99_07-12-2022 09_34_29\n",
      "Generating noise...\n",
      "- Keyboard\n",
      "-- \n",
      "- OCR\n",
      "-- \n",
      "- RandomCharReplace\n",
      "-- \n",
      "- CharSwap\n",
      "-- \n",
      "- WordSwap\n",
      "-- \n",
      "- WordSplit\n",
      "-- \n",
      "- Antonym\n",
      "-- \n",
      "- Synonym\n",
      "-- \n",
      "- Spelling\n",
      "-- \n",
      "- TfIdfWord\n",
      "-- \n",
      "- WordEmbeddings\n",
      "-- \n",
      "- ContextualWordEmbs\n",
      "-- \n",
      "Getting predictions from providers...\n",
      "- google\n",
      "-- Keyboard\n",
      "--- \n",
      "-- OCR\n",
      "--- \n",
      "-- RandomCharReplace\n",
      "--- \n",
      "-- CharSwap\n",
      "--- \n",
      "-- WordSwap\n",
      "--- \n",
      "-- WordSplit\n",
      "--- \n",
      "-- Antonym\n",
      "--- \n",
      "-- Synonym\n",
      "--- \n",
      "-- Spelling\n",
      "--- \n",
      "-- TfIdfWord\n",
      "--- \n",
      "-- WordEmbeddings\n",
      "--- \n",
      "-- ContextualWordEmbs\n",
      "--- \n",
      "- microsoft\n",
      "-- Keyboard\n",
      "--- \n",
      "-- OCR\n",
      "--- \n",
      "-- RandomCharReplace\n",
      "--- \n",
      "-- CharSwap\n",
      "--- \n",
      "-- WordSwap\n",
      "--- \n",
      "-- WordSplit\n",
      "--- \n",
      "-- Antonym\n",
      "--- \n",
      "-- Synonym\n",
      "--- \n",
      "-- Spelling\n",
      "--- \n",
      "-- TfIdfWord\n",
      "--- \n",
      "-- WordEmbeddings\n",
      "--- \n",
      "-- ContextualWordEmbs\n",
      "--- \n",
      "- amazon\n",
      "-- Keyboard\n",
      "--- \n",
      "-- OCR\n",
      "--- \n",
      "-- RandomCharReplace\n",
      "--- \n",
      "-- CharSwap\n",
      "--- \n",
      "-- WordSwap\n",
      "--- \n",
      "-- WordSplit\n",
      "--- \n",
      "-- Antonym\n",
      "--- \n",
      "-- Synonym\n",
      "--- \n",
      "-- Spelling\n",
      "--- \n",
      "-- TfIdfWord\n",
      "--- \n",
      "-- WordEmbeddings\n",
      "--- \n",
      "-- ContextualWordEmbs\n",
      "--- \n",
      "Calculating metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rocha\\repos\\ml_experiments\\fault_injection_mlaas\\visualization\\latex.py:107: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, noise_column_name] = pd.to_numeric(df[noise_column_name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results were saved to: ./outputs/experiment1/size99_07-12-2022 09_34_29\n"
     ]
    }
   ],
   "source": [
    "continue_from = continue_widget.value # Continue from previously ongoing progress. Insert the name of a /outputs/experiment1 folder\n",
    "\n",
    "def get_main_path(size):\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%m-%d-%Y %H_%M_%S\")\n",
    "    main_dir = './outputs/experiment1/size'+str(size)+'_' + timestamp\n",
    "    return main_dir\n",
    "\n",
    "def run_evaluation(sample_size: int,\n",
    "                  noise_levels: List[int] =[0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "                  noise_algorithms=[noises.no_noise, noises.RandomCharReplace, noises.Keyboard, noises.OCR],\n",
    "                  mlaas_providers=[ml_providers.google],\n",
    "                  continue_from=None):\n",
    "    if(continue_from):\n",
    "        main_path = './outputs/experiment1/'+continue_from\n",
    "        progress = progress_manager.load_progress(main_path)\n",
    "        x_dataset = read_dataset(main_path + '/data' + \"/dataset.xlsx\")\n",
    "        y_labels = read_dataset(main_path + '/data' + \"/labels.xlsx\")\n",
    "    else:\n",
    "        x_dataset, y_labels = data_sampling.get_dataset_sample('./Tweets_dataset.csv', sample_size)\n",
    "        main_path = get_main_path(len(x_dataset))\n",
    "        save_data_to_file(x_dataset, main_path + '/data', \"dataset\")\n",
    "        save_data_to_file(y_labels, main_path + '/data', \"labels\")\n",
    "        \n",
    "        progress = progress_manager.init_progress(main_path, noise_algorithms, noise_levels, mlaas_providers)\n",
    "    print(\"Results will be stored at: \", main_path)\n",
    "    print('Generating noise...')\n",
    "    progress = noise_insertion.generate_noised_data(x_dataset, main_path)\n",
    "\n",
    "    print('Getting predictions from providers...')\n",
    "    progress = ml_providers.get_prediction_results(main_path)\n",
    "\n",
    "    print('Calculating metrics...')\n",
    "    metrics_results = metrics.metrics(progress, y_labels, main_path)\n",
    "\n",
    "    noise_list = [0.0]\n",
    "    noise_list.extend(noise_levels)\n",
    "\n",
    "    visualization.plot_results(metrics_results, main_path + '/results', noise_list)\n",
    "\n",
    "    print(\"Results were saved to:\", main_path)\n",
    "\n",
    "run_evaluation(\n",
    "    sample_size,\n",
    "    noise_levels=noise_level,\n",
    "    noise_algorithms=noise_list,\n",
    "    mlaas_providers=[ml_providers.amazon, ml_providers.microsoft, ml_providers.google],\n",
    "    continue_from=continue_from\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d35a8d-6295-4a38-9c70-37d5be28072f",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f2c30-5614-4ca1-addc-1acb0ff15c32",
   "metadata": {},
   "source": [
    "## Importing code modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86529d03-82f5-42c3-a82d-21483ea5245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import TypedDict, List\n",
    "from datetime import datetime\n",
    "from data_sampling.data_sampling import DataSampling\n",
    "from progress import progress_manager\n",
    "from noise_insertion.percent_insertion import noises\n",
    "from noise_insertion import noise_insertion\n",
    "from mlaas_providers.providers import read_dataset\n",
    "from metrics import metrics\n",
    "from visualization import visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa20e114-47bb-4e9f-9b5e-9bc96a78d716",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8e59c3-987e-4392-bc39-fd992c7cfbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=100\n",
    "noise_level=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "word_counts = [\n",
    "    {\"min_width\": 12, \"max_width\": 12},\n",
    "    {\"min_width\": 19, \"max_width\": 19},\n",
    "    {\"min_width\": 23, \"max_width\": 23},\n",
    "]\n",
    "\n",
    "noise_algo = [\n",
    "    noises.Keyboard,\n",
    "    noises.OCR,\n",
    "    noises.RandomCharReplace,\n",
    "    noises.CharSwap,\n",
    "    noises.WordSwap,\n",
    "    noises.WordSplit,\n",
    "    noises.Antonym,\n",
    "    noises.Synonym,\n",
    "    noises.Spelling,\n",
    "    noises.TfIdfWord,\n",
    "    noises.WordEmbeddings,\n",
    "    noises.ContextualWordEmbs,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b701c73-2a43-49b6-8f21-c301298ad890",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d83e1-3f97-40c2-86b0-527b081181c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "To continue from previously ongoing progress insert the name of a /outputs/outputs/experiment2 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0b580f-de61-4385-96bc-68e09867a0a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aff8a23b07245ad99f7d136df001ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Continue from', placeholder='Type the name of a /outputs/experiment2 folder to…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "continue_widget = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Type the name of a /outputs/experiment2 folder to continue from',\n",
    "    description='Continue from',\n",
    "    disabled=False\n",
    ")\n",
    "continue_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "631762a4-dabb-40ec-9cad-8a6edd63d9b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size100_12-21-2022 20_53_00\n",
      "continue_from: ./outputs/experiment2/size100_12-21-2022 20_53_00\n",
      "Generating noise...\n",
      "- Keyboard\n",
      "-- \n",
      "- OCR\n",
      "-- \n",
      "- RandomCharReplace\n",
      "-- \n",
      "- CharSwap\n",
      "-- \n",
      "- WordSwap\n",
      "-- \n",
      "- WordSplit\n",
      "-- \n",
      "- Antonym\n",
      "-- \n",
      "- Synonym\n",
      "-- \n",
      "- Spelling\n",
      "-- \n",
      "- TfIdfWord\n",
      "-- \n",
      "- WordEmbeddings\n",
      "-- \n",
      "- ContextualWordEmbs\n",
      "-- \n",
      "Getting predictions from providers...\n",
      "- google\n",
      "-- Keyboard\n",
      "--- \n",
      "-- OCR\n",
      "--- \n",
      "-- RandomCharReplace\n",
      "--- \n",
      "-- CharSwap\n",
      "--- \n",
      "-- WordSwap\n",
      "--- \n",
      "-- WordSplit\n",
      "--- \n",
      "-- Antonym\n",
      "--- \n",
      "-- Synonym\n",
      "--- \n",
      "-- Spelling\n",
      "--- \n",
      "-- TfIdfWord\n",
      "--- \n",
      "-- WordEmbeddings\n",
      "--- \n",
      "-- ContextualWordEmbs\n",
      "--- \n",
      "- amazon\n",
      "-- Keyboard\n",
      "--- \n",
      "-- OCR\n",
      "--- \n",
      "-- RandomCharReplace\n",
      "--- \n",
      "-- CharSwap\n",
      "--- \n",
      "-- WordSwap\n",
      "--- \n",
      "-- WordSplit\n",
      "--- \n",
      "-- Antonym\n",
      "--- \n",
      "-- Synonym\n",
      "--- \n",
      "-- Spelling\n",
      "--- \n",
      "-- TfIdfWord\n",
      "--- \n",
      "-- WordEmbeddings\n",
      "--- \n",
      "-- ContextualWordEmbs\n",
      "--- \n",
      "- microsoft\n",
      "-- Keyboard\n",
      "--- \n",
      "-- OCR\n",
      "--- \n",
      "-- RandomCharReplace\n",
      "--- \n",
      "-- CharSwap\n",
      "--- \n",
      "-- WordSwap\n",
      "--- \n",
      "-- WordSplit\n",
      "--- \n",
      "-- Antonym\n",
      "--- \n",
      "-- Synonym\n",
      "--- \n",
      "-- Spelling\n",
      "--- \n",
      "-- TfIdfWord\n",
      "--- \n",
      "-- WordEmbeddings\n",
      "--- \n",
      "-- ContextualWordEmbs\n",
      "--- \n",
      "Calculating metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rocha\\repos\\ml_experiments\\fault_injection_mlaas\\visualization\\latex.py:107: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, noise_column_name] = pd.to_numeric(df[noise_column_name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./outputs/experiment2/size100_12-21-2022 20_53_00/[12-12]\n",
      "Generating noise...\n",
      "- Keyboard\n",
      "-- \n",
      "- OCR\n",
      "-- \n",
      "- RandomCharReplace\n",
      "-- \n",
      "- CharSwap\n",
      "-- \n",
      "- WordSwap\n",
      "-- \n",
      "- WordSplit\n",
      "-- \n",
      "- Antonym\n",
      "-- \n",
      "- Synonym\n",
      "-- \n",
      "- Spelling\n",
      "-- \n",
      "- TfIdfWord\n",
      "-- \n",
      "- WordEmbeddings\n",
      "-- \n",
      "- ContextualWordEmbs\n",
      "-- \n",
      "Getting predictions from providers...\n",
      "- google\n",
      "-- Keyboard\n",
      "--- \n",
      "-- OCR\n",
      "--- \n",
      "-- RandomCharReplace\n",
      "--- \n",
      "-- CharSwap\n",
      "--- \n",
      "-- WordSwap\n",
      "--- \n",
      "-- WordSplit\n",
      "--- \n",
      "-- Antonym\n",
      "--- \n",
      "-- Synonym\n",
      "--- \n",
      "-- Spelling\n",
      "--- \n",
      "-- TfIdfWord\n",
      "--- \n",
      "-- WordEmbeddings\n",
      "--- \n",
      "-- ContextualWordEmbs\n",
      "--- \n",
      "- amazon\n",
      "-- Keyboard\n",
      "--- \n",
      "-- OCR\n",
      "--- \n",
      "-- RandomCharReplace\n",
      "--- \n",
      "-- CharSwap\n",
      "--- \n",
      "-- WordSwap\n",
      "--- \n",
      "-- WordSplit\n",
      "--- \n",
      "-- Antonym\n",
      "--- \n",
      "-- Synonym\n",
      "--- \n",
      "-- Spelling\n",
      "--- \n",
      "-- TfIdfWord\n",
      "--- \n",
      "-- WordEmbeddings\n",
      "--- \n",
      "-- ContextualWordEmbs\n",
      "--- \n",
      "- microsoft\n",
      "-- Keyboard\n",
      "--- \n",
      "-- OCR\n",
      "--- \n",
      "-- RandomCharReplace\n",
      "--- \n",
      "-- CharSwap\n",
      "--- \n",
      "-- WordSwap\n",
      "--- \n",
      "-- WordSplit\n",
      "--- \n",
      "-- Antonym\n",
      "--- \n",
      "-- Synonym\n",
      "--- \n",
      "-- Spelling\n",
      "--- \n",
      "-- TfIdfWord\n",
      "--- \n",
      "-- WordEmbeddings\n",
      "--- \n",
      "-- ContextualWordEmbs\n",
      "--- \n",
      "Calculating metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rocha\\repos\\ml_experiments\\fault_injection_mlaas\\visualization\\latex.py:107: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, noise_column_name] = pd.to_numeric(df[noise_column_name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./outputs/experiment2/size100_12-21-2022 20_53_00/[19-19]\n",
      "Generating noise...\n",
      "- Keyboard\n",
      "-- \n",
      "- OCR\n",
      "-- \n",
      "- RandomCharReplace\n",
      "-- \n",
      "- CharSwap\n",
      "-- \n",
      "- WordSwap\n",
      "-- \n",
      "- WordSplit\n",
      "-- \n",
      "- Antonym\n",
      "-- \n",
      "- Synonym\n",
      "-- \n",
      "- Spelling\n",
      "-- \n",
      "- TfIdfWord\n",
      "-- \n",
      "- WordEmbeddings\n",
      "-- \n",
      "- ContextualWordEmbs\n",
      "-- \n",
      "Getting predictions from providers...\n",
      "- google\n",
      "-- Keyboard\n",
      "--- \n",
      "-- OCR\n",
      "--- \n",
      "-- RandomCharReplace\n",
      "--- \n",
      "-- CharSwap\n",
      "--- \n",
      "-- WordSwap\n",
      "--- \n",
      "-- WordSplit\n",
      "--- \n",
      "-- Antonym\n",
      "--- \n",
      "-- Synonym\n",
      "--- \n",
      "-- Spelling\n",
      "--- \n",
      "-- TfIdfWord\n",
      "--- \n",
      "-- WordEmbeddings\n",
      "--- \n",
      "-- ContextualWordEmbs\n",
      "--- \n",
      "- amazon\n",
      "-- Keyboard\n",
      "--- \n",
      "-- OCR\n",
      "--- \n",
      "-- RandomCharReplace\n",
      "--- \n",
      "-- CharSwap\n",
      "--- \n",
      "-- WordSwap\n",
      "--- \n",
      "-- WordSplit\n",
      "--- \n",
      "-- Antonym\n",
      "--- \n",
      "-- Synonym\n",
      "--- \n",
      "-- Spelling\n",
      "--- \n",
      "-- TfIdfWord\n",
      "--- \n",
      "-- WordEmbeddings\n",
      "--- \n",
      "-- ContextualWordEmbs\n",
      "--- \n",
      "- microsoft\n",
      "-- Keyboard\n",
      "--- \n",
      "-- OCR\n",
      "--- \n",
      "-- RandomCharReplace\n",
      "--- \n",
      "-- CharSwap\n",
      "--- \n",
      "-- WordSwap\n",
      "--- \n",
      "-- WordSplit\n",
      "--- \n",
      "-- Antonym\n",
      "--- \n",
      "-- Synonym\n",
      "--- \n",
      "-- Spelling\n",
      "--- \n",
      "-- TfIdfWord\n",
      "--- 0.9 , \n",
      "-- WordEmbeddings\n",
      "--- 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , \n",
      "-- ContextualWordEmbs\n",
      "--- 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , \n",
      "Calculating metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rocha\\repos\\ml_experiments\\fault_injection_mlaas\\visualization\\latex.py:107: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.loc[:, noise_column_name] = pd.to_numeric(df[noise_column_name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./outputs/experiment2/size100_12-21-2022 20_53_00/[23-23]\n",
      "['./outputs/experiment2/size100_12-21-2022 20_53_00/[12-12]', './outputs/experiment2/size100_12-21-2022 20_53_00/[19-19]', './outputs/experiment2/size100_12-21-2022 20_53_00/[23-23]']\n"
     ]
    }
   ],
   "source": [
    "continue_from = continue_widget.value \n",
    "print(continue_from)\n",
    "class Size(TypedDict):\n",
    "    min_width: int\n",
    "    max_width: int\n",
    "    \n",
    "def create_main_path(timestamp, size):\n",
    "    main_dir = f'./outputs/experiment2/size{str(size)}_{timestamp}'\n",
    "\n",
    "    Path(main_dir).mkdir(parents=True, exist_ok=True)\n",
    "    return main_dir\n",
    "\n",
    "def create_sub_path(main_path: str, min_width: int, max_width: int):\n",
    "    path = f'{main_path}/[{str(min_width)}-{str(max_width)}]'\n",
    "    \n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    Path(path+'/data').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    return path\n",
    "\n",
    "def prepare_execution(\n",
    "    continue_from: str,\n",
    "    timestamp: str,\n",
    "    sample_size: int,\n",
    "    sizes: List[Size],\n",
    "    noise_algorithms,\n",
    "    noise_levels,\n",
    "    mlaas_providers\n",
    "):\n",
    "    dataSampling = DataSampling()\n",
    "    if not len(continue_from) > 0:\n",
    "        main_path = create_main_path(timestamp, sample_size)\n",
    "    else:\n",
    "        continue_from = './outputs/experiment2/'+ continue_from\n",
    "        print(\"continue_from:\", continue_from)\n",
    "        main_path = continue_from\n",
    "    \n",
    "    sub_path_list = []\n",
    "    for size in sizes:\n",
    "        min_width = size['min_width']\n",
    "        max_width = size['max_width']\n",
    "        sub_path = create_sub_path(main_path, min_width, max_width)\n",
    "\n",
    "        data, labels = dataSampling.get_by_word_count('Tweets_dataset.csv',\n",
    "                                              sample_size,\n",
    "                                              min_width,\n",
    "                                              max_width)\n",
    "\n",
    "        path = Path(sub_path+\"/data/dataset.xlsx\")\n",
    "        if not path.is_file():\n",
    "            data.to_excel(sub_path+\"/data/dataset.xlsx\", 'data', index=False)\n",
    "        \n",
    "        path = Path(sub_path+\"/data/labels.xlsx\")\n",
    "        if not path.is_file():\n",
    "            labels.to_excel(sub_path+\"/data/labels.xlsx\", 'data', index=False)\n",
    "        sub_path_list.append(sub_path)\n",
    "        progress = progress_manager.init_progress(sub_path, noise_algorithms, noise_levels, mlaas_providers)\n",
    "    return sub_path_list\n",
    "\n",
    "def run_evaluation(noise_levels_units: List[int],\n",
    "                   continue_from: str,    \n",
    "):\n",
    "    main_path = continue_from\n",
    "    progress = progress_manager.load_progress(main_path)\n",
    "\n",
    "    x_dataset = read_dataset(main_path + '/data/dataset.xlsx')\n",
    "    y_labels = read_dataset(main_path + '/data/labels.xlsx')\n",
    "\n",
    "    print('Generating noise...')\n",
    "    progress = noise_insertion.generate_noised_data(x_dataset, main_path, noise_package=noises)\n",
    "\n",
    "    print('Getting predictions from providers...')\n",
    "    progress = ml_providers.get_prediction_results(main_path)\n",
    "\n",
    "    print('Calculating metrics...')\n",
    "    metrics_results = metrics.metrics(progress, y_labels, main_path)\n",
    "\n",
    "    noise_list = [0]\n",
    "    noise_list.extend(noise_levels_units)\n",
    "\n",
    "    visualization.plot_results(metrics_results, main_path + '/results', noise_list, percent_noise=True)\n",
    "\n",
    "    print(main_path)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%m-%d-%Y %H_%M_%S\")\n",
    "\n",
    "path_list = prepare_execution(continue_from,\n",
    "                          timestamp, \n",
    "                          sample_size,\n",
    "                          word_counts,\n",
    "                          noise_algo,\n",
    "                          noise_level,\n",
    "                          [ml_providers.google, ml_providers.amazon, ml_providers.microsoft])\n",
    "for path in path_list:\n",
    "    run_evaluation(noise_level, \n",
    "                   continue_from=path)\n",
    "print(path_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "98b75381e2c09cb53b84ee3c0ce4bb0c53f6c0b2bcf1d737d7d486e47dd4b47b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
